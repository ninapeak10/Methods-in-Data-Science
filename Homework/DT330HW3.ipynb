{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework #3 excerises 5, 6, 7, & 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We now examine the differences between LDA and QDA.\n",
    "\n",
    "(a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "QDA is expected to perform better than LDA on the training set. If the Bayes decision boundary is linear, LDA is expected to perform better than QDA on the test set.\n",
    "\n",
    "(b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "QDA is expected to perform better than LDA on the training set. If the Bayes decision boundary is non-linear,  QDA will also perform better on the test set.\n",
    "\n",
    "(c) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n",
    "\n",
    "In both cases of linear and non-linear Bayes decision boundary we expect the performance of QDA to improve relative to LDA, as n increases. In the linear boundary case, QDA will have a worse performance on the test set since its excessive flexibility will cause it to overfit, but this overfitting will decrease as n increases as the variance is reduced, and QDA will improve relative to LDA. For a non-linear Bayes decision boundary, LDA is biased and will not improve significantly past a certain sample size. QDA on the other hand, will see its variance reduced while benefitting from a more flexible model that captures the underlying non-linearity better leading to a closer fit.\n",
    "\n",
    "(d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n",
    "\n",
    "False. For a linear Bayes decision boundary, QDA is too flexible compared to LDA and the noise in the data will cause it to overfit. As the sample size increases the overfitting is reduced, but in general LDA is usually better since it is unbiased and less prone to fit the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Suppose we collect data for a group of students in a statistics class with variables X1 = hours studied,  X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, βˆ0 = −6, βˆ1 = 0.05, βˆ2 = 1.\n",
    "\n",
    "(a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.\n",
    "\n",
    "The probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class is 37.75%.\n",
    "\n",
    "(b) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?\n",
    "\n",
    "To have a 50% chance of getting an A in the class a student needs to study 50 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on X, last year’s percent profit. We examine a large number of companies and discover that the mean value of X for companies that issued a dividend was X ̄ = 10, while the mean for those that didn’t was X ̄ = 0. In addition, the variance of X for these two sets of companies was σˆ2 = 36. Finally, 80 % of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was X = 4 last year.\n",
    "\n",
    "The probability that a company will issue a dividend this year given that its percentage profit was X = 4 last year is 0.7571 or 75.71%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20 % on the training data and 30 % on the test data. Next we use 1-nearest neighbors (i.e. K = 1) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?\n",
    "\n",
    "The error rate on test data for the logistic regression is 30%. When we use the 1-nearest neighbor, the average error rate is 18%. For a 1-nearest neighbor model, we have training = 0 . This happens because for any training example, its nearest neighbor is always going to be itself. So, the method we should prefer to use for classification of new observations is the logistic regression method, even though its error rate on the training data is larger than the one for the 1-nearest neighbor model (18% vs. 0%)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
